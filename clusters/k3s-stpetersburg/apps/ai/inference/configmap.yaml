apiVersion: v1
kind: ConfigMap
metadata:
  name: deepseek-ocr-server
  namespace: ai
  labels:
    app: deepseek-ocr
data:
  server.py: |
    #!/usr/bin/env python3
    """
    DeepSeek-OCR FastAPI Server for KServe
    OpenAI-compatible OCR inference using HuggingFace Transformers

    This server provides a vLLM-compatible OpenAI API without using vLLM,
    allowing it to work on GB10 GPU (sm_121) which has Triton incompatibility.
    """
    import os
    import io
    import base64
    import logging
    import time
    import asyncio
    from typing import Optional, List, Dict, Any, AsyncGenerator
    from contextlib import asynccontextmanager

    from fastapi import FastAPI, HTTPException
    from fastapi.responses import StreamingResponse, JSONResponse
    from pydantic import BaseModel, Field
    import uvicorn

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    # Global model state
    model_state = {
        "model": None,
        "tokenizer": None,
        "Image": None,
        "torch": None,
        "loaded": False
    }


    def load_model():
        """Load the DeepSeek-OCR model on startup"""
        if model_state["loaded"]:
            return

        logger.info("Loading DeepSeek-OCR model...")

        from PIL import Image as PILImage
        import torch as torch_module
        from transformers import AutoModel, AutoTokenizer

        model_state["Image"] = PILImage
        model_state["torch"] = torch_module

        model_name = "deepseek-ai/DeepSeek-OCR"

        try:
            logger.info(f"Loading model from {model_name}...")
            model_state["model"] = AutoModel.from_pretrained(
                model_name,
                torch_dtype=torch_module.bfloat16,
                device_map="auto",
                trust_remote_code=True,
            )

            logger.info("Loading tokenizer...")
            model_state["tokenizer"] = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )

            model_state["loaded"] = True
            logger.info("DeepSeek-OCR model loaded successfully")

        except Exception as e:
            logger.error(f"Failed to load model: {e}", exc_info=True)
            raise


    @asynccontextmanager
    async def lifespan(app: FastAPI):
        """Load model on startup, cleanup on shutdown"""
        load_model()
        yield
        logger.info("Shutting down...")


    app = FastAPI(
        title="DeepSeek-OCR API",
        description="OpenAI-compatible OCR inference server (vLLM-compatible, Transformers-based)",
        version="1.0.0",
        lifespan=lifespan
    )


    # OpenAI API Models
    class ChatMessage(BaseModel):
        role: str
        content: str | List[Dict[str, Any]]


    class ChatCompletionRequest(BaseModel):
        model: str
        messages: List[ChatMessage]
        temperature: Optional[float] = 0.0
        max_tokens: Optional[int] = 8192
        stream: Optional[bool] = False


    class ChatCompletionResponse(BaseModel):
        id: str
        object: str = "chat.completion"
        created: int
        model: str
        choices: List[Dict[str, Any]]
        usage: Dict[str, int]


    class ModelInfo(BaseModel):
        id: str
        object: str = "model"
        created: int
        owned_by: str


    class ModelList(BaseModel):
        object: str = "list"
        data: List[ModelInfo]


    # Health and info endpoints
    @app.get("/")
    async def root():
        """Root endpoint"""
        return {
            "message": "DeepSeek-OCR API Server (Transformers-based, vLLM-compatible)",
            "docs": "/docs",
            "health": "/health",
            "models": "/v1/models"
        }


    @app.get("/health")
    async def health():
        """Health check endpoint"""
        if not model_state["loaded"]:
            raise HTTPException(status_code=503, detail="Model not loaded")
        return {"status": "healthy", "model": "deepseek-ocr"}


    @app.get("/v1/models")
    async def list_models() -> ModelList:
        """OpenAI-compatible models endpoint"""
        return ModelList(
            object="list",
            data=[
                ModelInfo(
                    id="deepseek-ocr",
                    object="model",
                    created=1730000000,
                    owned_by="deepseek-ai"
                )
            ]
        )


    def process_ocr_request(
        image_data,
        prompt_text: str,
        temperature: float,
        max_tokens: int
    ) -> tuple[str, int, int]:
        """Process OCR request and return response text and token counts"""

        if not model_state["loaded"]:
            raise HTTPException(status_code=503, detail="Model not loaded")

        model = model_state["model"]
        tokenizer = model_state["tokenizer"]

        logger.info(f"Processing OCR request: {prompt_text[:100]}...")

        # Save image to temporary file for model.infer()
        import tempfile
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
            image_data.save(tmp.name)
            tmp_path = tmp.name

        try:
            # Create temp output directory
            import tempfile
            import os
            tmp_output_dir = tempfile.mkdtemp()

            # Use DeepSeek-OCR's custom infer method
            response_text = model.infer(
                tokenizer=tokenizer,
                prompt=prompt_text,
                image_file=tmp_path,
                output_path=tmp_output_dir,
                base_size=1024,
                image_size=640,
                crop_mode=True,
                save_results=False
            )

            # Estimate token counts (approximate)
            prompt_tokens = len(tokenizer.encode(prompt_text))
            completion_tokens = len(tokenizer.encode(response_text))

            return response_text, prompt_tokens, completion_tokens

        finally:
            # Clean up temp files
            import os
            import shutil
            try:
                os.unlink(tmp_path)
            except Exception:
                pass
            try:
                shutil.rmtree(tmp_output_dir, ignore_errors=True)
            except Exception:
                pass


    @app.post("/v1/chat/completions")
    async def chat_completions(request: ChatCompletionRequest):
        """OpenAI-compatible chat completions endpoint"""

        try:
            messages = request.messages
            if not messages:
                raise HTTPException(status_code=400, detail="No messages provided")

            last_message = messages[-1]

            image_data = None
            prompt_text = ""

            # Parse message content
            if isinstance(last_message.content, list):
                for content in last_message.content:
                    if content.get("type") == "image_url":
                        image_url = content["image_url"]["url"]
                        if image_url.startswith("data:image"):
                            base64_data = image_url.split(",")[1]
                            image_bytes = base64.b64decode(base64_data)
                            image_data = model_state["Image"].open(
                                io.BytesIO(image_bytes)
                            ).convert("RGB")
                        else:
                            raise HTTPException(
                                status_code=400,
                                detail="Only base64 encoded images are supported"
                            )
                    elif content.get("type") == "text":
                        prompt_text = content["text"]
            else:
                prompt_text = last_message.content

            if image_data is None:
                raise HTTPException(
                    status_code=400,
                    detail="No image provided in message content"
                )

            # Ensure prompt has <image> token
            if not prompt_text:
                prompt_text = "<image>\nFree OCR."
            elif not prompt_text.startswith("<image>"):
                prompt_text = f"<image>\n{prompt_text}"

            # Process request
            response_text, prompt_tokens, completion_tokens = process_ocr_request(
                image_data,
                prompt_text,
                request.temperature,
                request.max_tokens
            )

            # Return OpenAI-compatible response
            response = ChatCompletionResponse(
                id=f"chatcmpl-{os.urandom(12).hex()}",
                object="chat.completion",
                created=int(time.time()),
                model=request.model,
                choices=[
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": response_text
                        },
                        "finish_reason": "stop"
                    }
                ],
                usage={
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": prompt_tokens + completion_tokens
                }
            )

            if request.stream:
                raise HTTPException(
                    status_code=400,
                    detail="Streaming not yet supported"
                )

            return response

        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Error processing request: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=str(e))


    if __name__ == "__main__":
        port = int(os.getenv("PORT", "8080"))
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=port,
            log_level="info"
        )
