apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: deepseek-ocr
  namespace: ai
  labels:
    app: deepseek-ocr
  annotations:
    serving.kserve.io/deploymentMode: "RawDeployment"
    serving.kserve.io/autoscalerClass: "hpa"
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 2
    scaleTarget: 80
    scaleMetric: concurrency
    containerConcurrency: 1
    timeout: 600
    containers:
    - name: predictor
      image: nvcr.io/nvidia/pytorch:25.01-py3
      command:
        - python3
        - /app/server.py
      env:
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            name: hf-secret
            key: HF_TOKEN
      - name: HF_HOME
        value: /tmp/huggingface
      - name: TRANSFORMERS_CACHE
        value: /tmp/huggingface
      - name: TORCH_HOME
        value: /tmp/torch
      - name: PORT
        value: "8080"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: PYTORCH_CUDA_ALLOC_CONF
        value: "max_split_size_mb:512"
      - name: PYTHONPATH
        value: "/app"
      ports:
      - containerPort: 8080
        protocol: TCP
        name: http
      resources:
        requests:
          nvidia.com/gpu: "1"
        limits:
          nvidia.com/gpu: "1"
      volumeMounts:
      - name: server-code
        mountPath: /app
      - name: hf-cache
        mountPath: /tmp/huggingface
      - name: torch-cache
        mountPath: /tmp/torch
      - name: devshm
        mountPath: /dev/shm
      - name: pip-packages
        mountPath: /pip-packages
      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 300
        periodSeconds: 30
        timeoutSeconds: 10
        failureThreshold: 3
      readinessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 300
        periodSeconds: 10
        timeoutSeconds: 10
        failureThreshold: 3
      startupProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 60
        periodSeconds: 30
        timeoutSeconds: 10
        failureThreshold: 20
    initContainers:
    - name: install-deps
      image: nvcr.io/nvidia/pytorch:25.01-py3
      command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "Installing Python dependencies to user site..."
          # NOTE: Using --user to install AFTER system packages (where CUDA torch is)
          # This ensures NGC's CUDA PyTorch is used instead of CPU PyTorch from PyPI
          pip install --user --no-cache-dir \
            transformers==4.46.3 \
            tokenizers==0.20.3 \
            Pillow \
            einops \
            easydict \
            addict \
            fastapi==0.115.0 \
            uvicorn[standard]==0.32.0 \
            pydantic==2.9.2 \
            python-multipart

          # Install accelerate without dependencies to avoid torch conflict
          pip install --user --no-cache-dir --no-deps \
            accelerate==1.6.0

          echo "Dependencies installed successfully"
          python3 -m site
      volumeMounts:
      - name: pip-packages
        mountPath: /pip-packages
    volumes:
    - name: server-code
      configMap:
        name: deepseek-ocr-server
        defaultMode: 0755
    - name: hf-cache
      emptyDir:
        sizeLimit: 20Gi
    - name: torch-cache
      emptyDir:
        sizeLimit: 10Gi
    - name: devshm
      emptyDir:
        medium: Memory
        sizeLimit: 8Gi
    - name: pip-packages
      emptyDir:
        sizeLimit: 5Gi
